# -*- coding: utf-8 -*-
"""Datamining_CA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dleK2oaocxaqLOky1h_gsoYK0FN5CiUi

# Importing Necessary Libraries
"""

import pandas as pd
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import re, nltk
import nltk
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.svm import LinearSVC
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from imblearn.over_sampling import SMOTE

"""# Data Preprocessing

"""

# Load the SMS Spam Collection dataset
file_path = '/content/SMSSpamCollection'
df = pd.read_csv(file_path, sep='\t', header=None, names=['label', 'message'])

df.head()

df.info()

df.shape

# Convert label to binary
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

df.head()

stop = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stop.update(punctuation)

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

# Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

# Removing URL's
def remove_urls(text):
    return re.sub(r'http\S+', '', text)

# Remove punctuation
def remove_punctuation(text):
    no_punct = "".join([c for c in text if c not in string.punctuation])
    return no_punct

# Removing the stopwords from text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop:
            if i.strip().isalpha():
                final_text.append(i.strip())
    return " ".join(final_text)

# Create Master Function
def denoise_text(text):
    text = strip_html(text)
    text = remove_stopwords(text)
    text = remove_urls(text)
    text = remove_punctuation(text)
    text = text.lower()
    return text

df['cleaned_message']=df['message'].apply(denoise_text)

df.head()

df.isnull().sum()

#check if there is any DUPLICATE values
df.duplicated().sum()

df.shape

df = df.drop_duplicates(keep='first')

#displaying the edited dataframe
df

df['label'].value_counts()

"""# Exploratory Data Analysis - EDA

"""

plt.pie(df['label'].value_counts(),  labels = ['NOT SPAM', 'SPAM'], autopct = '%0.2f', radius = 0.8)
plt.show()

"""Now we will be analysing the number of alphabets/words/sentences being used in the TEXT for this, will create 3 new columns: (1) no. of characters (2) no. of words (3) no. of sentences in SMS

"""

#creating a new column with count of characters
df['countCharacters'] = df['cleaned_message'].apply(len)

#creating a new column with count of words
df['countWords'] = df['cleaned_message'].apply(lambda i:len(nltk.word_tokenize(i)))
#'word_tokenize' function takes a string of text as input and returns a list of words

#creating a new column with count of sentences
df['countSentences'] = df['cleaned_message'].apply(lambda i:len(nltk.sent_tokenize(i)))
#'sent_tokenize' function takes a string of text as input and returns a list of sentences

df.head()

df[['countCharacters', 'countWords', 'countSentences']].describe()

df[df['label'] == 0][['countCharacters', 'countWords', 'countSentences']].describe()

df[df['label'] == 1][['countCharacters', 'countWords', 'countSentences']].describe()

plt.figure(figsize = (15, 5))
sns.histplot(df[df['label'] == 0]['countCharacters'], color = "blue")
sns.histplot(df[df['label'] == 1]['countCharacters'], color = "black")

plt.figure(figsize = (15, 5))
sns.histplot(df[df['label'] == 0]['countWords'], color = "blue")
sns.histplot(df[df['label'] == 1]['countWords'], color = "black")

sns.pairplot(df, hue='label')

numerical_features = ['countCharacters', 'countWords', 'countSentences', 'label']
correlation_matrix = df[numerical_features].corr()

# Display the correlation matrix
print(correlation_matrix)

sns.heatmap(df[numerical_features].corr(), annot=True)

wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')

#creating a wordcloud for the SPAM messages
spamWC = wc.generate(df[df['label'] == 1]['cleaned_message'].str.cat(sep=" "))

#creating figure and displaying
plt.figure(figsize=(12, 6))
plt.imshow(spamWC)

#creating a wordcloud for the SPAM messages
spamWC = wc.generate(df[df['label'] == 0]['cleaned_message'].str.cat(sep=" "))

#creating figure and displaying
plt.figure(figsize=(12, 6))
plt.imshow(spamWC)

tfidf = TfidfVectorizer(stop_words='english')
X = tfidf.fit_transform(df['cleaned_message'])

y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""**Balancing**"""

smote = SMOTE()
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print('-----------------------------------------')
print('Synthetic sample class distribution: \n')
print(pd.Series(y_train_resampled).value_counts())

"""# Model Building

**Multinomial Naive Bayes**
"""

# Running cross-validation
nbc_clf = MultinomialNB()
data_tfidf = X
Y = y
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0

# Variables to store predictions and true labels for all folds
all_Y_pred = []
all_Y_test = []

for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)
    # Convert the sparse matrix to a dense array before indexing
    X_train, X_test = data_tfidf[train_index].toarray(), data_tfidf[test_index].toarray()
    # Use iloc to index the Series based on integer positions
    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]
    #The line below was changed to use Y_train generated in current fold.
    nbc_clf.fit(X_train, Y_train) # Fitting NBC - Removed extra toarray() call
    Y_pred = nbc_clf.predict(X_test)

    # Append predictions and true labels for this fold
    all_Y_pred.extend(Y_pred)
    all_Y_test.extend(Y_test)

    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy
    print("Cross-validation accuracy: ", score)
    scores.append(score) # appending cross-validation accuracy for each iteration

nbc_mean_accuracy = np.mean(scores)
print("Mean cross-validation accuracy: ", nbc_mean_accuracy)

# Calculate and print classification report for all folds combined
print(classification_report(all_Y_test, all_Y_pred))

cm = confusion_matrix(all_Y_test, all_Y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""**Support Vector Classifier**"""

# Running cross-validation
model = LinearSVC() # kernel = 'linear' and C = 1
data_tfidf = X
Y = y
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0

# Variables to store predictions and true labels for all folds
all_Y_pred = []
all_Y_test = []

for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)
    # Convert the sparse matrix to a dense array before indexing
    X_train, X_test = data_tfidf[train_index].toarray(), data_tfidf[test_index].toarray()
    # Use iloc to index the Series based on integer positions
    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]
    model.fit(X_train, Y_train) # Fitting SVC
    Y_pred = model.predict(X_test)

    # Append predictions and true labels for this fold
    all_Y_pred.extend(Y_pred)
    all_Y_test.extend(Y_test)

    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy
    print("Cross-validation accuracy: ", score)
    scores.append(score) # appending cross-validation accuracy for each iteration

mean_accuracy = np.mean(scores)
print("Mean cross-validation accuracy: ", mean_accuracy)

# Calculate and print classification report for all folds combined
print(classification_report(all_Y_test, all_Y_pred))

cm = confusion_matrix(all_Y_test, all_Y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

svc_probs = model.decision_function(X_test) # For SVC, use decision_function
# Predict probabilities for MNB
mnb_probs = nbc_clf.predict_proba(X_test)[:, 1] # For MNB, use predict_proba


# ROC Curve for SVC
fpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test[:svc_probs.shape[0]], svc_probs)
roc_auc_svc = auc(fpr_svc, tpr_svc)

# ROC Curve for MNB
fpr_mnb, tpr_mnb, thresholds_mnb = roc_curve(y_test[:mnb_probs.shape[0]], mnb_probs)
roc_auc_mnb = auc(fpr_mnb, tpr_mnb)

# Plot ROC curves
plt.figure(figsize=(8, 6))
plt.plot(fpr_svc, tpr_svc, color='darkorange', lw=2, label='SVC (AUC = %0.2f)' % roc_auc_svc)
plt.plot(fpr_mnb, tpr_mnb, color='navy', lw=2, label='MNB (AUC = %0.2f)' % roc_auc_mnb)
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()